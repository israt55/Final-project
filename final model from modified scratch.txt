# Final CNN

# This model is built from scratch.
# The dataset used was after the re-split.

# Create CNN

classifier = Sequential()
classifier.add(Conv2D(64, (5,5), strides=(2,2), padding='valid', activation='relu', input_shape=(208,176,1)))
classifier.add(MaxPooling2D(2))
classifier.add(Conv2D(128, (5,5), strides=(2,2), padding='valid', activation='relu'))
classifier.add(Conv2D(256, (5,5), strides=(2,2), padding='valid', activation='relu'))
classifier.add(MaxPooling2D(2))
classifier.add(Flatten())
classifier.add(Dense(64, activation='relu'))
classifier.add(Dropout(0.4))
classifier.add(Dense(32, activation='relu'))
classifier.add(Dropout(0.4))
classifier.add(Dense(4, activation='softmax'))
classifier.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(),metrics=['AUC','accuracy','categorical_accuracy'])
checkpoint_cb = ModelCheckpoint("checkpoint_model.h5", save_best_only=True)
es = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True, verbose=1)
classifier.summary()

# Fit CNN

history = classifier.fit(train_x, train_y, validation_data=(val_x, val_y), batch_size=10, epochs =50, callbacks=[es, checkpoint_cb])

# Plot accuracy and loss curves

plt.plot(history.history['categorical_accuracy'], label="Training Accuracy")
plt.plot(history.history['val_categorical_accuracy'], label="Validation Accuracy")
plt.legend(loc="best")
plt.show()
plt.plot(history.history['loss'], label="Training Loss")
plt.plot(history.history['val_loss'], label="Validation Loss")
plt.legend(loc="best")
plt.show()

# Evaluate on test set and create confusion matrix

test_results = classifier.evaluate(x=test_x, y=test_y)

pred_test = classifier.predict(test_x, verbose = 1)
pred_test = onehot.inverse_transform(pred_test)
real_val = onehot.inverse_transform(test_y)

cm = confusion_matrix(real_val, pred_test, labels=['ND', 'VMILD', 'MILD', 'MOD'])
cm_df = pd.DataFrame(cm, columns=['ND', 'VMILD', 'MILD', 'MOD'], index=['ND', 'VMILD', 'MILD', 'MOD'])
cm_df.columns.name = 'Predicted'
cm_df.index.name = 'Actual'
sns.heatmap(cm_df, annot=True, fmt='g', cmap='Greens')

##########################################################################
# Experimenting with Different Prediction Thresholds
#
# Here we attempt to modify the thresholds in order to
# avoid costly misclassifications.
##########################################################################

# load in the model checkpoint from the final CNN mode

model = keras.models.load_model('checkpoint_model.h5')
model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics =['AUC','acc'])

# Recreate confusion matrix with standard classification rules
# i.e. predict class with highest probability

pred_test = model.predict(x=test_x)
pred_test = model.predict(test_x, verbose = 1)
pred_test = onehot.inverse_transform(pred_test)
real_val = onehot.inverse_transform(test_y)
real_val = onehot.inverse_transform(test_y)
cm_tl = confusion_matrix(real_val, pred_test, labels=['ND', 'VMILD', 'MILD', 'MOD'])
cm_tl_df = pd.DataFrame(cm_tl, columns=['ND', 'VMILD', 'MILD', 'MOD'], index=['ND', 'VMILD', 'MILD', 'MOD'])
cm_tl_df.columns.name = 'Predicted'
cm_tl_df.index.name = 'Actual'
sns.heatmap(cm_tl_df, annot=True, fmt='g', cmap='Greens')

# Calculate individual prediction probabilities
# This time only predict ND (non-demented) if its probability
# is a certain threshold higher than the next highest probability.
# In the code below, this threshold is set at 0.9

pred_prob = model.predict(test_x)
predictions=[]
classes=['MILD','MOD','ND','VMILD']
for probs in pred_prob:
  inds = np.argsort(probs)
  if inds[len(inds)-1] != 2:
    predictions.append(classes[inds[len(inds)-1]])
  else:
    index_top = inds[len(inds)-1]
    index_second = inds[len(inds)-2]
    if probs[index_top] - probs[index_second] < 0.9:
      predictions.append(classes[inds[len(inds)-2]])
    else:
      predictions.append(classes[inds[len(inds)-1]])

# show new confusion matrix

real_val = onehot.inverse_transform(test_y)
cm_tl = confusion_matrix(real_val, predictions, labels=['ND', 'VMILD', 'MILD', 'MOD'])
cm_tl_df = pd.DataFrame(cm_tl, columns=['ND', 'VMILD', 'MILD', 'MOD'], index=['ND', 'VMILD', 'MILD', 'MOD'])
cm_tl_df.columns.name = 'Predicted'
cm_tl_df.index.name = 'Actual'
sns.heatmap(cm_tl_df, annot=True, fmt='g', cmap='Greens')

##########################################################################
# Visualizing the CNN
#
# In this section, we try to visualize different stages of the CNN
# as well as find the most important sections of the images
##########################################################################

# Prepare data for visualizing activation

train_x_tmp = np.array(train_x).reshape(4096,208,176)
val_x_tmp = np.array(val_x).reshape(1024,208,176)
test_x_tmp = np.array(test_x).reshape(1280,208,176)

layer_outputs = [layer.output for layer in model.layers]
activation_model = Model(inputs=model.input, outputs=layer_outputs)
activations = activation_model.predict(train_x_tmp[0].reshape(1,208,176,1))

# Define function to display activation layer

def display_activation(activations, col_size, row_size, act_index): 
    activation = activations[act_index]
    activation_index=0
    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))
    for row in range(0,row_size):
        for col in range(0,col_size):
            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')
            activation_index += 1

# Visualize randomly selected image

plt.imshow(train_x[0][:,:,0], cmap='gray')

# Display activations for each of the three convolutional layers
# Will display in 8x8 grid but this can be changed

display_activation(activations, 8, 8, 1)
display_activation(activations, 8, 8, 2)
display_activation(activations, 8, 8, 3)

# Define function to show heatmap of image.
# Sections of the image that are more important will be shown with a red overlay
# on top of the actual image

def show_heatmap(file_path):
  img = imread(file_path, as_gray=True)
  cv2_imshow(cv2.imread(file_path)) # Visualize image
  img = img.astype('float32')
  img /= 255
  x = image.img_to_array(img)
  x=np.reshape(x, (1, 208, 176, 1))
  preds = model.predict(x)
  with tf.GradientTape() as tape:
    last_conv_layer = model.get_layer('conv2d_2')
    iterate = Model([model.inputs], [model.output, last_conv_layer.output])
    model_out, last_conv_layer = iterate(x)
    class_out = model_out[:, np.argmax(model_out[0])]
    grads = tape.gradient(class_out, last_conv_layer)
    pooled_grads = keras.backend.mean(grads, axis=(0, 1, 2))
  
  heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)
  heatmap = np.maximum(heatmap, 0)
  heatmap /= np.max(heatmap)
  heatmap = heatmap.reshape((10, 8))
  plt.matshow(heatmap)
  plt.show()

  img = cv2.imread(file_path)
  INTENSITY = 0.5
  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
  heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)
  img = heatmap * INTENSITY + img

  cv2_imshow(img)

# Show heatmap for image
# show_heatmap takes a file path as a parameter

show_heatmap('/content/drive/MyDrive/final-project/dataset/train/VeryMildDemented/verymildDem10.jpg')